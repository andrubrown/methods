{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from tethne.readers import zotero\n",
    "import matplotlib.pyplot as plt\n",
    "from helpers import normalize_token, filter_token\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature selection: keywords\n",
    "\n",
    "A major problem-area in text mining is determining the thematic or topical content of texts. One of the most basic problems in this area is to identify the terms in a text -- \"keywords\" -- that most accurately represents its distinctive thematic characteristics.\n",
    "\n",
    "In this notebook, we will use Dunning's log-likelihood statistic to identify keywords for individual documents in a collection of texts. It is fairly typical that methods used for statistical analysis are also used for information extraction and classification. \n",
    "\n",
    "We'll use the Embryo Project corpus from earlier notebooks. Recall that the plain text documents are stored separately from their metadata -- this is the format that you would expect from a Zotero RDF export."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "documents = nltk.corpus.PlaintextCorpusReader('../data/EmbryoProjectTexts/files', 'https.+')\n",
    "metadata = zotero.read('../data/EmbryoProjectTexts', index_by='link', follow_links=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first step is to count up all of the words in each of the documents. This conditional frequency distribution should look familiar by now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wordcounts_per_document = nltk.ConditionalFreqDist([\n",
    "    (fileid, normalize_token(token))\n",
    "     for fileid in documents.fileids()\n",
    "     for token in documents.words(fileids=[fileid])\n",
    "     if filter_token(token)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy import sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "627\n"
     ]
    }
   ],
   "source": [
    "# We pick a single \"focal\" document that we want to characterize.\n",
    "focal_fileid = documents.fileids()[3]\n",
    "\n",
    "# Since this procedure will involve numerical matrices, we\n",
    "#  need to map documents and words onto row and column indices.\n",
    "#  These \"dictionaries\" will help us to keep track of those\n",
    "#  mappings.\n",
    "document_index = {}    # Maps int -> fileid (str).\n",
    "vocabulary = {}        # Maps int -> word (str).\n",
    "lookup = {}            # Reverse map for vocabulary (word (str) -> int).\n",
    "\n",
    "# Containers for sparse data.\n",
    "I = []          # Document vector.\n",
    "J = []          # Word vector.\n",
    "data = []       # Word count vector.\n",
    "labels = []     # Vector of labels; either the URI of interest, or \"Other\".\n",
    "\n",
    "# Here we transform the ConditionalFrequencyDist into three vectors (I, J, data)\n",
    "#  that sparsely describe the document-word count matrix.\n",
    "for i, (fileid, counts) in enumerate(wordcounts_per_document.iteritems()):\n",
    "    document_index[i] = fileid\n",
    "    for token, count in counts.iteritems():\n",
    "        # Removing low-frequency terms is optional, but speeds things up\n",
    "        #  quite a bit for this demonstration.\n",
    "        if count < 3:\n",
    "            continue\n",
    "        \n",
    "        # get() lets us \n",
    "        j = lookup.get(token, len(vocabulary))\n",
    "        vocabulary[j] = token\n",
    "        lookup[token] = j\n",
    "        \n",
    "        I.append(i)\n",
    "        J.append(j)\n",
    "        data.append(count)\n",
    "    labels.append(fileid if fileid == focal_fileid else 'Other')\n",
    "    print '\\r', i,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sparse_matrix = sparse.coo_matrix((data, (I, J)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(628, 7033)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparse_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "keyness, _ = chi2(sparse_matrix, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ranking = np.argsort(keyness)[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_, words = zip(*sorted(vocabulary.items(), key=lambda i: i[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "words = np.array(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "keywords = words[ranking]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'occurred', 937.50239234449759),\n",
       " (u'fertilized', 796.9760972679635),\n",
       " (u'laparoscopy', 551.63727500569598),\n",
       " (u'successfully', 521.70309745656004),\n",
       " (u'icsi', 508.64564303320293),\n",
       " (u'usually', 371.41531100478466),\n",
       " (u'ivf', 363.83650540492647),\n",
       " (u'louise', 327.0183882165307),\n",
       " (u'brown', 316.68266707825279),\n",
       " (u'vitro', 315.16533758639025),\n",
       " (u'egg', 281.59206727562702),\n",
       " (u'successful', 265.95285262598281),\n",
       " (u'implant', 263.59603554340396),\n",
       " (u'attempt', 243.6516746411483),\n",
       " (u'fertilization', 229.0308302413566),\n",
       " (u'mature', 210.84376950280841),\n",
       " (u'uterus', 197.3138013152433),\n",
       " (u'steptoe', 192.15841200051727),\n",
       " (u'rabbit', 157.06855439642325),\n",
       " (u'wall', 155.78951925267714)]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zip(keywords[:20], keyness[ranking][:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_keywords(fileid, n=20):\n",
    "    print '\\r', fileid,\n",
    "    document_index = {}    # Maps int -> fileid (str).\n",
    "    vocabulary = {}    # Maps int -> word (str).\n",
    "    lookup = {}    # Reverse map for vocabulary (word (str) -> int).\n",
    "\n",
    "    I = []\n",
    "    J = []\n",
    "    data = []\n",
    "    labels = []\n",
    "\n",
    "    for i, (key, counts) in enumerate(wordcounts_per_document.iteritems()):\n",
    "        document_index[i] = key\n",
    "        for token, count in counts.iteritems():\n",
    "            if count < 3:\n",
    "                continue\n",
    "            j = lookup.get(token, len(vocabulary))\n",
    "            vocabulary[j] = token\n",
    "            lookup[token] = j\n",
    "\n",
    "            I.append(i)\n",
    "            J.append(j)\n",
    "            data.append(count)\n",
    "        labels.append(key if key == fileid else 'Other')\n",
    "        \n",
    "    sparse_matrix = sparse.coo_matrix((data, (I, J)))\n",
    "    \n",
    "    keyness, _ = chi2(sparse_matrix, labels)\n",
    "    ranking = np.argsort(keyness)[::-1]\n",
    "    _, words = zip(*sorted(vocabulary.items(), key=lambda i: i[0]))\n",
    "    words = np.array(words)\n",
    "    keywords = words[ranking]\n",
    "    return keywords[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https--____hpsrepository.asu.edu__handle__10776__8688.txt\n"
     ]
    }
   ],
   "source": [
    "keywords = [extract_keywords(fileid) for fileid in documents.fileids()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
